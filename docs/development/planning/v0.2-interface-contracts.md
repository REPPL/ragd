# v0.2 Interface Contracts

Python Protocols and dataclasses defining contracts between v0.2 components.

**Purpose:** Enable independent implementation and testing of components.

---

## Table of Contents

- [PDF Processing](#pdf-processing)
- [OCR Pipeline](#ocr-pipeline)
- [Metadata Extraction](#metadata-extraction)
- [Export/Import](#exportimport)
- [Feature Detection](#feature-detection)

---

## PDF Processing

### Quality Detection

```python
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Protocol


class PDFQuality(Enum):
    """Classification of PDF quality for pipeline routing."""

    DIGITAL_NATIVE = "digital_native"      # Has good text layer
    COMPLEX_LAYOUT = "complex_layout"      # Tables, multi-column
    SCANNED = "scanned"                    # Needs OCR
    MIXED = "mixed"                        # Some pages digital, some scanned


@dataclass(frozen=True)
class QualityAssessment:
    """Result of PDF quality analysis."""

    quality: PDFQuality
    text_coverage: float              # 0.0-1.0, % of pages with text
    scan_probability: float           # 0.0-1.0, likelihood of being scanned
    has_tables: bool
    has_multi_column: bool
    page_count: int
    recommended_pipeline: str         # "fast", "structure", "ocr"
    confidence: float                 # Confidence in assessment


class PDFQualityDetector(Protocol):
    """Protocol for PDF quality detection."""

    def assess(self, pdf_path: Path) -> QualityAssessment:
        """Analyse PDF and return quality assessment."""
        ...
```

### PDF Processor

```python
from dataclasses import dataclass, field
from pathlib import Path
from typing import Protocol


@dataclass
class ExtractedTable:
    """A table extracted from a PDF."""

    page_number: int
    table_index: int                  # 0-indexed within page
    markdown: str                     # Table as Markdown
    rows: int
    cols: int
    confidence: float


@dataclass
class ExtractedContent:
    """Content extracted from a PDF."""

    text: str                         # Full text in reading order
    pages: list[str]                  # Text per page
    tables: list[ExtractedTable]
    metadata: dict[str, str]          # PDF metadata (title, author, etc.)
    processing_time_ms: int


class PDFProcessor(Protocol):
    """Protocol for PDF text extraction."""

    def extract(self, pdf_path: Path) -> ExtractedContent:
        """Extract text and structure from PDF."""
        ...

    @property
    def name(self) -> str:
        """Name of this processor (e.g., 'PyMuPDF', 'Docling')."""
        ...

    @property
    def supports_tables(self) -> bool:
        """Whether this processor can extract tables."""
        ...

    @property
    def supports_layout(self) -> bool:
        """Whether this processor handles complex layouts."""
        ...
```

### Pipeline Factory

```python
from pathlib import Path


class PDFPipelineFactory:
    """Factory for selecting appropriate PDF processor."""

    def get_processor(
        self,
        pdf_path: Path,
        *,
        force_pipeline: str | None = None,
    ) -> PDFProcessor:
        """Get appropriate processor based on PDF quality.

        Args:
            pdf_path: Path to PDF file
            force_pipeline: Override automatic selection
                           ("fast", "structure", "ocr")

        Returns:
            PDFProcessor instance

        Raises:
            DependencyError: If required optional dependency not installed
        """
        ...
```

---

## OCR Pipeline

### OCR Result

```python
from dataclasses import dataclass
from pathlib import Path
from typing import Protocol


@dataclass(frozen=True)
class BoundingBox:
    """Bounding box for detected text."""

    x1: int
    y1: int
    x2: int
    y2: int


@dataclass
class OCRResult:
    """Single text detection result."""

    text: str
    confidence: float                 # 0.0-1.0
    bbox: BoundingBox | None = None
    line_number: int | None = None


@dataclass
class PageOCRResult:
    """OCR results for a single page."""

    page_number: int
    results: list[OCRResult]
    average_confidence: float
    processing_time_ms: int

    @property
    def full_text(self) -> str:
        """Concatenated text from all results."""
        return "\n".join(r.text for r in self.results)
```

### OCR Engine Protocol

```python
class OCREngine(Protocol):
    """Protocol for OCR engines."""

    def ocr_image(self, image_path: Path) -> list[OCRResult]:
        """Run OCR on a single image."""
        ...

    def ocr_pdf_page(
        self,
        pdf_path: Path,
        page_number: int,
    ) -> PageOCRResult:
        """Run OCR on a single PDF page."""
        ...

    @property
    def name(self) -> str:
        """Name of this OCR engine."""
        ...

    @property
    def supports_gpu(self) -> bool:
        """Whether this engine can use GPU acceleration."""
        ...
```

### OCR Pipeline

```python
@dataclass
class OCRConfig:
    """Configuration for OCR processing."""

    min_confidence: float = 0.3       # Minimum confidence threshold
    use_gpu: bool = False             # Enable GPU acceleration
    language: str = "en"              # OCR language
    fallback_enabled: bool = True     # Use fallback engine if primary fails


class OCRPipeline(Protocol):
    """Protocol for OCR pipeline with fallback."""

    def process_pdf(
        self,
        pdf_path: Path,
        config: OCRConfig | None = None,
    ) -> list[PageOCRResult]:
        """Process entire PDF with OCR."""
        ...

    def process_page(
        self,
        pdf_path: Path,
        page_number: int,
        config: OCRConfig | None = None,
    ) -> PageOCRResult:
        """Process single PDF page with OCR."""
        ...

    @property
    def primary_engine(self) -> str:
        """Name of primary OCR engine."""
        ...

    @property
    def fallback_engine(self) -> str | None:
        """Name of fallback engine, if configured."""
        ...
```

---

## Metadata Extraction

### Document Metadata (Dublin Core)

```python
from dataclasses import dataclass, field
from datetime import datetime


@dataclass
class DocumentMetadata:
    """Dublin Core-based metadata with RAG extensions."""

    # Schema version for migration
    ragd_schema_version: str = "2.0"

    # Dublin Core Core Elements
    dc_title: str = ""
    dc_creator: list[str] = field(default_factory=list)
    dc_subject: list[str] = field(default_factory=list)
    dc_description: str = ""
    dc_date: datetime | None = None
    dc_type: str = ""                 # "Research Paper", "Report", etc.
    dc_format: str = ""               # MIME type
    dc_identifier: str = ""           # DOI, ISBN, etc.
    dc_language: str = "en"

    # RAG Extensions
    ragd_source_path: str = ""
    ragd_source_hash: str = ""
    ragd_chunk_count: int = 0
    ragd_ingestion_date: datetime | None = None
    ragd_quality_score: float = 0.0
    ragd_tags: list[str] = field(default_factory=list)
    ragd_project: str = ""

    def to_dict(self) -> dict:
        """Convert to dictionary for storage."""
        ...

    @classmethod
    def from_dict(cls, data: dict) -> "DocumentMetadata":
        """Create from stored dictionary."""
        ...
```

### Extracted Metadata

```python
@dataclass
class ExtractedKeyword:
    """A keyword extracted from document text."""

    keyword: str
    score: float                      # 0.0-1.0, relevance score
    source: str = "keybert"           # Extraction method


@dataclass
class ExtractedEntity:
    """A named entity extracted from document text."""

    text: str
    label: str                        # PERSON, ORG, GPE, DATE, etc.
    start_char: int
    end_char: int
    confidence: float


@dataclass
class ExtractedMetadata:
    """All metadata extracted from a document."""

    # From PDF metadata
    pdf_title: str | None = None
    pdf_author: str | None = None
    pdf_creation_date: datetime | None = None

    # From NLP extraction
    keywords: list[ExtractedKeyword] = field(default_factory=list)
    entities: list[ExtractedEntity] = field(default_factory=list)
    detected_language: str = "en"
    language_confidence: float = 0.0
```

### Metadata Extractor Protocol

```python
from pathlib import Path
from typing import Protocol


class MetadataExtractor(Protocol):
    """Protocol for metadata extraction."""

    def extract(
        self,
        text: str,
        pdf_path: Path | None = None,
    ) -> ExtractedMetadata:
        """Extract metadata from text and optionally PDF file.

        Args:
            text: Document text content
            pdf_path: Optional path to PDF for native metadata

        Returns:
            ExtractedMetadata with all extracted information
        """
        ...

    def extract_keywords(
        self,
        text: str,
        top_n: int = 10,
        diversity: float = 0.5,
    ) -> list[ExtractedKeyword]:
        """Extract keywords using KeyBERT."""
        ...

    def extract_entities(
        self,
        text: str,
        entity_types: list[str] | None = None,
    ) -> list[ExtractedEntity]:
        """Extract named entities using spaCy."""
        ...

    def detect_language(
        self,
        text: str,
    ) -> tuple[str, float]:
        """Detect language, returns (language_code, confidence)."""
        ...
```

---

## Export/Import

### Archive Format

```python
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path


@dataclass
class ArchiveManifest:
    """Manifest describing archive contents."""

    version: str = "1.0"              # Archive format version
    ragd_version: str = ""            # ragd version that created archive
    created_at: datetime = field(default_factory=datetime.now)
    document_count: int = 0
    chunk_count: int = 0
    embedding_count: int = 0
    embedding_model: str = ""
    embedding_dimensions: int = 0
    has_embeddings: bool = True
    checksum: str = ""                # SHA-256 of contents


@dataclass
class ExportOptions:
    """Options for archive export."""

    include_embeddings: bool = True
    include_config: bool = True
    filter_tags: list[str] | None = None
    filter_project: str | None = None
    filter_since: datetime | None = None
    compression: str = "gzip"         # "gzip", "lz4", "none"
```

### Export Engine Protocol

```python
class ExportEngine(Protocol):
    """Protocol for knowledge base export."""

    def export(
        self,
        output_path: Path,
        options: ExportOptions | None = None,
    ) -> ArchiveManifest:
        """Export knowledge base to archive.

        Args:
            output_path: Path to output archive (.tar.gz)
            options: Export configuration

        Returns:
            Manifest describing exported content
        """
        ...

    def inspect(self, archive_path: Path) -> ArchiveManifest:
        """Inspect archive without extracting."""
        ...

    def verify(self, archive_path: Path) -> bool:
        """Verify archive integrity."""
        ...
```

### Import Engine Protocol

```python
from enum import Enum


class ConflictResolution(Enum):
    """How to handle import conflicts."""

    SKIP = "skip"                     # Keep existing, skip conflicting
    OVERWRITE = "overwrite"           # Replace with imported
    KEEP_BOTH = "keep_both"           # Import as new (duplicate)


@dataclass
class ImportOptions:
    """Options for archive import."""

    conflict_resolution: ConflictResolution = ConflictResolution.SKIP
    regenerate_embeddings: bool = False
    dry_run: bool = False


@dataclass
class ImportResult:
    """Result of import operation."""

    documents_imported: int
    documents_skipped: int
    documents_overwritten: int
    chunks_imported: int
    embeddings_regenerated: bool
    conflicts: list[str]              # List of conflicting doc IDs


class ImportEngine(Protocol):
    """Protocol for knowledge base import."""

    def import_archive(
        self,
        archive_path: Path,
        options: ImportOptions | None = None,
    ) -> ImportResult:
        """Import archive into knowledge base.

        Args:
            archive_path: Path to archive file
            options: Import configuration

        Returns:
            ImportResult with statistics
        """
        ...

    def preview(self, archive_path: Path) -> tuple[ArchiveManifest, list[str]]:
        """Preview import, returns (manifest, conflicts)."""
        ...
```

---

## Feature Detection

### Feature Availability

```python
from dataclasses import dataclass


@dataclass(frozen=True)
class FeatureStatus:
    """Status of an optional feature."""

    available: bool
    name: str
    install_command: str | None = None
    extra_steps: str | None = None    # e.g., "Run: python -m spacy download..."


class FeatureDetector:
    """Detect availability of optional features."""

    @property
    def docling(self) -> FeatureStatus:
        """Check Docling availability."""
        ...

    @property
    def paddleocr(self) -> FeatureStatus:
        """Check PaddleOCR availability."""
        ...

    @property
    def easyocr(self) -> FeatureStatus:
        """Check EasyOCR availability."""
        ...

    @property
    def ocr(self) -> FeatureStatus:
        """Check any OCR availability."""
        ...

    @property
    def keybert(self) -> FeatureStatus:
        """Check KeyBERT availability."""
        ...

    @property
    def spacy(self) -> FeatureStatus:
        """Check spaCy availability."""
        ...

    @property
    def spacy_model(self) -> FeatureStatus:
        """Check spaCy model availability."""
        ...

    @property
    def metadata(self) -> FeatureStatus:
        """Check full metadata extraction availability."""
        ...

    @property
    def pyarrow(self) -> FeatureStatus:
        """Check PyArrow availability."""
        ...

    def all_features(self) -> dict[str, FeatureStatus]:
        """Get status of all optional features."""
        ...
```

### Dependency Error

```python
class DependencyError(Exception):
    """Raised when an optional dependency is required but missing."""

    def __init__(
        self,
        message: str,
        feature: str,
        install_command: str | None = None,
        extra_steps: str | None = None,
    ):
        self.feature = feature
        self.install_command = install_command
        self.extra_steps = extra_steps
        super().__init__(message)

    def user_message(self) -> str:
        """Format user-friendly error message."""
        msg = f"{self.args[0]}\n"
        if self.install_command:
            msg += f"\nInstall with: {self.install_command}"
        if self.extra_steps:
            msg += f"\nThen run: {self.extra_steps}"
        return msg
```

---

## Usage Patterns

### Graceful Degradation Example

```python
from ragd.features import FeatureDetector, DependencyError

detector = FeatureDetector()


def get_pdf_processor(assessment: QualityAssessment) -> PDFProcessor:
    """Select processor based on quality and available features."""
    if assessment.quality == PDFQuality.SCANNED:
        if not detector.ocr.available:
            raise DependencyError(
                "OCR required for scanned documents.",
                feature="ocr",
                install_command="pip install 'ragd[ocr]'",
            )
        return OCRPipeline()

    if assessment.quality == PDFQuality.COMPLEX_LAYOUT:
        if detector.docling.available:
            return DoclingProcessor()
        # Graceful degradation
        logger.warning(
            "Docling not available, using PyMuPDF. "
            "Table extraction may be limited."
        )
        return PyMuPDFProcessor()

    return PyMuPDFProcessor()
```

### Lazy Loading Example

```python
class MetadataExtractorImpl:
    """Implementation with lazy-loaded models."""

    def __init__(self):
        self._keybert: KeyBERT | None = None
        self._nlp: Language | None = None

    def _get_keybert(self) -> KeyBERT:
        if self._keybert is None:
            from keybert import KeyBERT
            # Share embedding model with ragd
            self._keybert = KeyBERT(model="all-MiniLM-L6-v2")
        return self._keybert

    def _get_spacy(self) -> Language:
        if self._nlp is None:
            import spacy
            self._nlp = spacy.load("en_core_web_sm")
        return self._nlp
```

---

## Implementation Checklist

When implementing these interfaces:

- [ ] Create `src/ragd/pdf/` module with processors
- [ ] Create `src/ragd/ocr/` module with engines
- [ ] Create `src/ragd/metadata/` module with extractors
- [ ] Create `src/ragd/export/` module with engines
- [ ] Create `src/ragd/features.py` for detection
- [ ] Add `DependencyError` to `src/ragd/exceptions.py`
- [ ] Update `ragd doctor` to show feature status
- [ ] Write tests for each protocol

---

## Related Documentation

- [ADR-0019: PDF Processing](../decisions/adrs/0019-pdf-processing.md)
- [ADR-0023: Metadata Schema Evolution](../decisions/adrs/0023-metadata-schema-evolution.md)
- [ADR-0024: Optional Dependencies](../decisions/adrs/0024-optional-dependencies.md)
- [Docling Integration Guide](../research/docling-integration-guide.md)
- [PaddleOCR Integration Guide](../research/paddleocr-integration-guide.md)
- [NLP Library Integration](../research/nlp-library-integration.md)

---

**Status**: Ready for implementation
