# Milestone: v0.5.0 - Chat

Conversational interface with local LLM integration.

## Goal

Transform ragd from a retrieval tool into a conversational knowledge assistant. Using local LLMs via Ollama, users can have natural conversations about their documents with full context preservation and quality evaluation.

## Theme

**"Ask and Learn"** - From retrieval to conversation.

## Use Cases Addressed

| Use Case | Description | Status |
|----------|-------------|--------|
| Q&A | Ask questions, get answers from your docs | ðŸ“‹ Planned |
| Summarisation | Summarise documents or topics | ðŸ“‹ Planned |
| Comparison | Compare information across documents | ðŸ“‹ Planned |
| Follow-up | Refine queries through conversation | ðŸ“‹ Planned |

## Features Included

| Feature | Description | Status |
|---------|-------------|--------|
| [F-013](../features/planned/F-013-ragas-evaluation.md) | RAGAS Evaluation | ðŸ“‹ Planned |
| [F-014](../features/planned/F-014-agentic-rag.md) | Agentic RAG (CRAG, Self-RAG) | ðŸ“‹ Planned |
| F-020: Ollama LLM Integration | Local LLM for generation | ðŸ“‹ Planned |
| Chat History | Conversation context preservation | ðŸ“‹ Planned |
| Streaming | Stream responses for better UX | ðŸ“‹ Planned |

## Dependencies

- **v0.4.0** - Multi-Modal must be complete
- **Ollama** - Required for LLM generation

## Technology Stack

| Component | Technology |
|-----------|------------|
| LLM Backend | Ollama |
| Generation Models | Llama 3.2, Qwen 2.5, Mistral |
| Evaluation | RAGAS / DeepEval with local alternatives |
| Chat History | SQLite or ChromaDB metadata |
| Streaming | Typer streaming output |

## Success Criteria

- [ ] Natural conversation with knowledge base
- [ ] Answers cite sources from documents
- [ ] Conversation context preserved across turns
- [ ] Response quality measurable via RAGAS
- [ ] Agentic RAG improves response quality by 20%
- [ ] Streaming responses for better UX
- [ ] Works fully offline with local models

## Non-Goals for v0.5

- Multi-user support â†’ Future
- Web UI â†’ v1.0
- Cloud LLM integration â†’ Never (privacy-first)

## Draft Release Notes

### ragd v0.5.0 - Chat

**Ask and learn.**

Stop searching. Start asking. ragd now speaks your language.

**New Features:**

- **Chat Mode** - Conversational interface with your knowledge
- **Local LLM** - Powered by Ollama, fully private
- **Agentic RAG** - Self-correcting retrieval and generation
- **Quality Evaluation** - RAGAS metrics for response quality
- **Streaming** - Real-time response generation

**Getting Started:**

```bash
# Ensure Ollama is running with a model
ollama pull llama3.2:3b

# Start a chat session
ragd chat
> What authentication methods are discussed in my documents?
> Can you summarise the pros and cons?
> Compare JWT vs session-based auth

# Single question mode
ragd ask "summarise the main points about security"
```

**What's Next:**

v0.6 focuses on storage: flexible backends and the exciting LEANN project.

---

## Related Documentation

- [v0.4.0 - Multi-Modal](./v0.4.0.md) - Prerequisite milestone
- [State-of-the-Art RAG](../research/state-of-the-art-rag.md)
- [State-of-the-Art Local RAG](../research/state-of-the-art-local-rag.md)
- [State-of-the-Art Evaluation](../research/state-of-the-art-evaluation.md)
- [F-013: RAGAS Evaluation](../features/planned/F-013-ragas-evaluation.md)
- [F-014: Agentic RAG](../features/planned/F-014-agentic-rag.md)

---

**Status**: Planned
